# Multimodal-Deepfake-Detection-and-Explainability

## Overview

This project proposes a multimodal deepfake detection framework that integrates both video and audio to check for manipulations, since most content available on OSNs is multimodal in nature. The methodology is to capture cross-modal inconsistencies, such as mismatched lip-syncing, or audio that does not align, any varied voice modulations, and visual inconsistencies. 
This project aims to develop a robust system for detecting deepfakes using a multimodal approach. It incorporates machine learning techniques with model explainability, allowing users to understand the reasoning behind detection outcomes.

## Features

- **Multimodal Detection**: Utilises multimodal data to enhance detection accuracy.
- **Three-Class Classification**: Detects and categorises deepfakes into three distinct classes based on combinations of manipulations
- **Explainability**: Provides insights into the model's decision-making process, helping users understand how detections are made.
- **Interactive Jupyter Notebook**: A notebook is provided for experimentation and demonstration of detection capabilities

Data Src: https://sites.google.com/view/fakeavcelebdash-lab/home
